{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "OCR entire page instead of individual elements\n",
    "subject1\n",
    "    - Validate that OCR’ing entire page is faster than OCR’ing individual blocks\n",
    "    - Validate that the text returned by OCR’ing entire page will is as accurate as the individual blocks"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import pdf2image\n",
    "\n",
    "cur_dir = os.getcwd()\n",
    "base_dir = os.path.join(cur_dir, os.pardir, os.pardir)\n",
    "example_docs_dir = os.path.join(base_dir, \"sample-docs\")\n",
    "\n",
    "# folder path to save temporary outputs\n",
    "test_dir = os.path.join(cur_dir, \"tmp\")\n",
    "os.makedirs(test_dir, exist_ok=True)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% paths\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number_of_pages: 16\n",
      "individual_page_images:\n",
      "\timage1 - size: (1700, 2200)\n",
      "\timage2 - size: (1700, 2200)\n",
      "\timage3 - size: (1700, 2200)\n"
     ]
    }
   ],
   "source": [
    "import tempfile\n",
    "\n",
    "filename = \"layout-parser-paper.pdf\"\n",
    "f_path = os.path.join(example_docs_dir, filename)\n",
    "\n",
    "filename_without_extension = os.path.splitext(filename)[0]\n",
    "sub_test_dir = os.path.join(test_dir, filename_without_extension)\n",
    "os.makedirs(sub_test_dir, exist_ok=True)\n",
    "\n",
    "with tempfile.TemporaryDirectory() as tmpdir:\n",
    "    images = pdf2image.convert_from_path(f_path, output_folder=tmpdir)\n",
    "\n",
    "\n",
    "individual_page_img_paths = []\n",
    "individual_page_images = []\n",
    "for i, image in enumerate(images):\n",
    "    # Save the image to a file\n",
    "    # img_path = os.path.join(sub_test_dir, f\"page_{i+1}.jpg\")\n",
    "    # image.save(img_path)\n",
    "    # individual_page_img_paths.append(img_path)\n",
    "\n",
    "    individual_page_images.append(image)\n",
    "\n",
    "n_pages = len(individual_page_images)\n",
    "print(f\"number_of_pages: {n_pages}\")\n",
    "\n",
    "print(\"individual_page_images:\")\n",
    "for i, image in enumerate(individual_page_images[:3]):\n",
    "    print(f\"\\timage{i+1} - size: {image.size}\")\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model_type: UnstructuredObjectDetectionModel\n"
     ]
    }
   ],
   "source": [
    "# OCR'ing individual blocks\n",
    "\n",
    "from engine import run_ocr_with_layout_detection\n",
    "\n",
    "inferred_layouts, infer_time_individual, text_individual = run_ocr_with_layout_detection(\n",
    "    images=individual_page_images,\n",
    "    output_dir=sub_test_dir,\n",
    ")\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# OCR'ing entire page\n",
    "\n",
    "from examples.ocr.engine import run_ocr\n",
    "\n",
    "infer_time_entire, text_entire = run_ocr(image_paths=individual_page_img_paths)\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n",
    "print(\"Processing Time (OCR'ing individual blocks)\")\n",
    "print(f\"\\ttotal_infer_time: {infer_time_individual}\")\n",
    "print(f\"\\tavg_infer_time_per_page: {infer_time_individual / n_pages }\")\n",
    "\n",
    "print(\"Processing Time (OCR'ing entire page)\")\n",
    "print(f\"\\ttotal_infer_time: {infer_time_entire}\")\n",
    "print(f\"\\tavg_infer_time_per_page: {infer_time_entire / n_pages}\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# calculate similarity ratio\n",
    "from difflib import SequenceMatcher\n",
    "similarity_ratio = SequenceMatcher(None, text_individual, text_entire).ratio()\n",
    "\n",
    "print(f\"similarity_ratio: {similarity_ratio}\")\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import nltk\n",
    "\n",
    "# Download the required resources (run this once)\n",
    "nltk.download('punkt')\n",
    "\n",
    "# Tokenize the text into words\n",
    "word_list_by_individual_blocks = nltk.word_tokenize(text_individual)\n",
    "print(\"n_word_list_individual_blocks:\", len(word_list_by_individual_blocks))\n",
    "word_sets_individual = set(list(word_list_by_individual_blocks))\n",
    "print(f\"n_word_sets_individual_blocks: {len(word_sets_individual)}\")\n",
    "# print(\"word_sets_merged:\", word_sets_merged)\n",
    "\n",
    "word_list_entire = nltk.word_tokenize(text_entire)\n",
    "print(\"n_word_list_individual:\", len(word_list_entire))\n",
    "word_sets_entire = set(list(word_list_entire))\n",
    "print(f\"n_word_sets_individual: {len(word_sets_entire)}\")\n",
    "# print(\"word_sets_individual:\", word_sets_individual)\n",
    "\n",
    "# Find unique elements using difference\n",
    "print(\"diff_elements:\")\n",
    "print(f\"{word_sets_individual - word_sets_entire}\\n\")\n",
    "print(word_sets_entire - word_sets_individual)\n"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
